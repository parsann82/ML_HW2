{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ChC3RF8meAlK"
   },
   "source": [
    "<h1 align=\"center\">Introduction to Machine Learning - 25737-2</h1>\n",
    "<h4 align=\"center\">Dr. R. Amiri</h4>\n",
    "<h4 align=\"center\">Sharif University of Technology, Spring 2024</h4>\n",
    "\n",
    "\n",
    "**<font color='red'>Plagiarism is strongly prohibited!</font>**\n",
    "\n",
    "\n",
    "**Student Name**: Parsa Norouzinezhad\n",
    "\n",
    "**Student ID**: 400102182\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IraiR0SbeDi_"
   },
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nRQjwWC3eDnc"
   },
   "source": [
    "**Task:** Implement your own Logistic Regression model, and test it on the given dataset of Logistic_question.csv!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data = pd.read_csv(\"Logistic_question.csv\")\n",
    "X = data.drop('Target', axis=1)\n",
    "y = data['Target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, learning_rate=0.01, num_iterations=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_iterations = num_iterations\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.weights = np.zeros(X.shape[1])\n",
    "        self.bias = 0\n",
    "\n",
    "        for _ in range(self.num_iterations):\n",
    "            linear_model = np.dot(X, self.weights) + self.bias\n",
    "            y_predicted = self.sigmoid(linear_model)\n",
    "\n",
    "            dw = (1 / len(X)) * np.dot(X.T, (y_predicted - y))\n",
    "            db = (1 / len(X)) * np.sum(y_predicted - y)\n",
    "\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "\n",
    "    def predict(self, X):\n",
    "        linear_model = np.dot(X, self.weights) + self.bias\n",
    "        y_predicted = self.sigmoid(linear_model)\n",
    "        return np.round(y_predicted)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S-i-oubUlZ6e"
   },
   "source": [
    "**Task:** Test your model on the given dataset. You must split your data into train and test, with a 0.2 split, then normalize your data using X_train data. Finally, report 4 different evaluation metrics of the model on the test set. (You might want to first make the Target column binary!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0KXzIy_2u-pG",
    "outputId": "9625f7e2-abb1-4591-c0fa-843525e0ffd6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ji0RXNGKv1pa"
   },
   "source": [
    "**Question:** What are each of your used evaluation metrics? And for each one, mention situations in which they convey more data on the model performance in specific tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ldveD35twRRZ"
   },
   "source": [
    "**Your answer:**\n",
    "Accuracy is a good metric for balanced datas and measures the proportion of correct predictions out of the total predictions made by the model.Precision could be helpful by measuring the proportion of true positive predictions out of all positive predictions made by the model especially when the cost of false positives is high.However, Sensitivity measures the proportion of true positive predictions out of all actual positive instances in the dataset.It's important when the cost of false negatives is high. F1 score is the harmonic mean of precision and recall. It provides a balance between precision and recall.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ZCeRHZSw-mh"
   },
   "source": [
    "**Task:** Now test the built-in function of Python for Logistic Regression, and report all the same metrics used before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Vb5lRSQXDLR3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1 Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "y_train_binary = np.where(y_train > 0, 1, 0)\n",
    "y_test_binary = np.where(y_test > 0, 1, 0)\n",
    "\n",
    "# Train the logistic regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train_binary)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluation Metrics\n",
    "accuracy = accuracy_score(y_test_binary, y_pred)\n",
    "precision = precision_score(y_test_binary, y_pred)\n",
    "recall = recall_score(y_test_binary, y_pred)\n",
    "f1 = f1_score(y_test_binary, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RCvIymmMy_ji"
   },
   "source": [
    "**Question:** Compare your function with the built-in function. On the matters of performance and parameters. Briefly explain what the parameters of the built-in function are and how they affect the model's performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EY0ohM16z3De"
   },
   "source": [
    "**Your answer:** \n",
    "When comparing the custom logistic regression function with the built-in function, the built-in function being optimized and well-tested, especially for handling large datasets. While the custom function is lightweight and suitable for learning purposes, the built-in function offers more advanced parameters that significantly affect the model's performance. These parameters include penalty, controlling the type of regularization (L1 or L2), C, which inversely determines the regularization strength with smaller values to prevent overfitting, setting the maximum number of iterations for optimization, which influences convergence and training times. The custom function, on the other hand, allows for the customization of learning rate and number of iterations, affecting the speed and convergence of the optimization algorithm (gradient descent). Overall, the built-in function's comprehensive parameterization provides more control and flexibility, crucial for achieving optimal model performance across diverse datasets and problem domains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ClMqoYlr2kr7"
   },
   "source": [
    "# Multinomial Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ukvlqDe52xP5"
   },
   "source": [
    "**Task:** Implement your own Multinomial Logistic Regression model. Your model must be able to handle any number of labels!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "5Ir-_hFt286t"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class MultinomialLogisticRegression:\n",
    "    def __init__(self, learning_rate=0.01, num_iterations=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_iterations = num_iterations\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.num_classes = None\n",
    "\n",
    "    def softmax(self, z):\n",
    "        exp_scores = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "        return exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.num_classes = len(np.unique(y))\n",
    "        self.weights = np.zeros((n_features, self.num_classes))\n",
    "        self.bias = np.zeros(self.num_classes)\n",
    "        y = y.astype(int)\n",
    "        one_hot_y = np.eye(self.num_classes)[y]\n",
    "        for _ in range(self.num_iterations):\n",
    "            linear_model = np.dot(X, self.weights) + self.bias\n",
    "            y_predicted = self.softmax(linear_model)\n",
    "            dw = (1 / n_samples) * np.dot(X.T, (y_predicted - one_hot_y))\n",
    "            db = (1 / n_samples) * np.sum(y_predicted - one_hot_y, axis=0)\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "\n",
    "    def predict(self, X):\n",
    "        linear_model = np.dot(X, self.weights) + self.bias\n",
    "        y_predicted = self.softmax(linear_model)\n",
    "        return np.argmax(y_predicted, axis=1)\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "model = MultinomialLogisticRegression(learning_rate=0.1, num_iterations=1000)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "y_pred = model.predict(X_test_scaled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zPQ3Rtay3Y2_"
   },
   "source": [
    "**Task:** Test your model on the given dataset. Do the same as the previous part, but here you might want to first make the Target column quantized into $i$ levels. Change $i$ from 2 to 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "9aP4QJPq29B3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantization with 2 levels:\n",
      "Target\n",
      "0    209\n",
      "1    191\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Accuracy: 0.9375\n",
      "Precision: 0.937891604010025\n",
      "Recall: 0.9375\n",
      "F1 Score: 0.9375490196078431\n",
      "--------------------------------------------------\n",
      "Quantization with 3 levels:\n",
      "Target\n",
      "1    136\n",
      "0    136\n",
      "2    128\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Accuracy: 0.85\n",
      "Precision: 0.8466666666666667\n",
      "Recall: 0.85\n",
      "F1 Score: 0.8476997578692493\n",
      "--------------------------------------------------\n",
      "Quantization with 4 levels:\n",
      "Target\n",
      "0    113\n",
      "3     98\n",
      "1     96\n",
      "2     93\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Accuracy: 0.7125\n",
      "Precision: 0.7374851778656126\n",
      "Recall: 0.7125\n",
      "F1 Score: 0.7209412537537538\n",
      "--------------------------------------------------\n",
      "Quantization with 5 levels:\n",
      "Target\n",
      "3    85\n",
      "1    84\n",
      "0    81\n",
      "4    75\n",
      "2    75\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Accuracy: 0.6625\n",
      "Precision: 0.700703046953047\n",
      "Recall: 0.6625\n",
      "F1 Score: 0.6679573777329196\n",
      "--------------------------------------------------\n",
      "Quantization with 6 levels:\n",
      "Target\n",
      "2    73\n",
      "0    69\n",
      "1    67\n",
      "5    66\n",
      "3    63\n",
      "4    62\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Accuracy: 0.6\n",
      "Precision: 0.5961309523809524\n",
      "Recall: 0.6\n",
      "F1 Score: 0.5957621082621082\n",
      "--------------------------------------------------\n",
      "Quantization with 7 levels:\n",
      "Target\n",
      "0    64\n",
      "3    59\n",
      "2    59\n",
      "1    58\n",
      "6    57\n",
      "5    52\n",
      "4    51\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Accuracy: 0.525\n",
      "Precision: 0.5335071301247772\n",
      "Recall: 0.525\n",
      "F1 Score: 0.5267376893939394\n",
      "--------------------------------------------------\n",
      "Quantization with 8 levels:\n",
      "Target\n",
      "1    63\n",
      "3    56\n",
      "4    51\n",
      "0    50\n",
      "7    49\n",
      "6    49\n",
      "5    42\n",
      "2    40\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Accuracy: 0.45\n",
      "Precision: 0.44170430264180266\n",
      "Recall: 0.45\n",
      "F1 Score: 0.43791702037351443\n",
      "--------------------------------------------------\n",
      "Quantization with 9 levels:\n",
      "Target\n",
      "0    49\n",
      "4    47\n",
      "7    47\n",
      "2    46\n",
      "3    45\n",
      "5    44\n",
      "8    42\n",
      "1    41\n",
      "6    39\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Accuracy: 0.4375\n",
      "Precision: 0.4374313186813186\n",
      "Recall: 0.4375\n",
      "F1 Score: 0.42559839585701653\n",
      "--------------------------------------------------\n",
      "Quantization with 10 levels:\n",
      "Target\n",
      "4    44\n",
      "6    43\n",
      "3    43\n",
      "7    42\n",
      "2    41\n",
      "0    41\n",
      "1    40\n",
      "8    39\n",
      "9    36\n",
      "5    31\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Accuracy: 0.3375\n",
      "Precision: 0.3774404761904762\n",
      "Recall: 0.3375\n",
      "F1 Score: 0.3257175390399074\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "data = pd.read_csv(\"Logistic_question.csv\")\n",
    "\n",
    "for i in range(2, 11):\n",
    "    quantized_target = pd.qcut(data['Target'], q=i, labels=False)\n",
    "    print(f\"Quantization with {i} levels:\")\n",
    "    print(quantized_target.value_counts())\n",
    "    print()\n",
    "    X = data.iloc[:, :-1].values\n",
    "    y = quantized_target.values\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    model = MultinomialLogisticRegression(learning_rate=0.1, num_iterations=1000)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F1 Score:\", f1)\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Of2sHl5Z4dXi"
   },
   "source": [
    "**Question:** Report for which $i$ your model performs best. Describe and analyze the results! You could use visualizations or any other method!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cRLERDAr4wnS"
   },
   "source": [
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wT43jGKV6CBZ"
   },
   "source": [
    "# Going a little further!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vo9uGo0R6GZo"
   },
   "source": [
    "First we download Adult income dataset from Kaggle! In order to do this create an account on this website, and create an API. A file named kaggle.json will be downloaded to your device. Then use the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "id": "o-vrjYBF7u1E",
    "outputId": "b274bc6e-4c35-4ad8-f17b-9e69f7d92923"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\parsa nrz\\OneDrive\\Desktop\\IML_CHW2\\Q2.ipynb Cell 22\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/parsa%20nrz/OneDrive/Desktop/IML_CHW2/Q2.ipynb#X30sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgoogle\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcolab\u001b[39;00m \u001b[39mimport\u001b[39;00m files\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/parsa%20nrz/OneDrive/Desktop/IML_CHW2/Q2.ipynb#X30sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m files\u001b[39m.\u001b[39mupload()  \u001b[39m# Use this to select the kaggle.json file from your computer\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/parsa%20nrz/OneDrive/Desktop/IML_CHW2/Q2.ipynb#X30sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m get_ipython()\u001b[39m.\u001b[39msystem(\u001b[39m'\u001b[39m\u001b[39mmkdir -p ~/.kaggle\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "from google.colab import files\n",
    "files.upload()  # Use this to select the kaggle.json file from your computer\n",
    "!mkdir -p ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5i6u6_1v8ftX"
   },
   "source": [
    "Then use this code to automatically download the dataset into Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XjyVaVKF29Hx",
    "outputId": "15d0b1a2-c806-4102-abbc-12545237e218"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'kaggle' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'unzip' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets download -d wenruliu/adult-income-dataset\n",
    "!unzip /content/adult-income-dataset.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EXQnbZwt8rJK"
   },
   "source": [
    "**Task:** Determine the number of null entries!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JtuEx6QW29c1",
    "outputId": "43397bec-0622-4dc4-de2b-c65be00e4503"
   },
   "outputs": [],
   "source": [
    "# Your code goes here!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JpEcBdTUAYVN"
   },
   "source": [
    "**Question:** In many widely used datasets there are a lot of null entries. Propose 5 methods by which, one could deal with this problem. Briefly explain how do you decide which one to use in this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l1u1pBHuAsSg"
   },
   "source": [
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eHhH-hkpAxFf"
   },
   "source": [
    "**Task:** Handle null entries using your best method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "5fVwWcjK29fk",
    "outputId": "c21a6adf-1e6c-46d0-dd61-79d1710272c1"
   },
   "outputs": [],
   "source": [
    "# Your code goes here!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "43k5cTorCJaV"
   },
   "source": [
    "**Task:** Convert categorical features to numerical values. Split the dataset with 80-20 portion. Normalize all the data using X_train. Use the built-in Logistic Regression function and GridSearchCV to train your model, and report the parameters, train and test accuracy of the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Agj18Lcd-vyZ",
    "outputId": "69e132a9-0249-4a21-c8f3-45247c1e17dc"
   },
   "outputs": [],
   "source": [
    "# Your code goes here!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Lzr2lqXDQ1T"
   },
   "source": [
    "**Task:** To try a different route, split X_train into $i$ parts, and train $i$ separate models on these parts. Now propose and implement 3 different *ensemble methods* to derive the global models' prediction for X_test using the results(not necessarily predictions!) of the $i$ models. Firstly, set $i=10$ to find the method with the best test accuracy(the answer is not general!). You must Use your own Logistic Regression model.(You might want to modify it a little bit for this part!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K9D1jlstF9nF"
   },
   "outputs": [],
   "source": [
    "# Your code goes here!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9QS9HYJ5FW1T"
   },
   "source": [
    "**Question:** Explain your proposed methods and the reason you decided to use them!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6hCBQuAeF46a"
   },
   "source": [
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jjSREvg4FTHf"
   },
   "source": [
    "**Task:** Now, for your best method, change $i$ from 2 to 100 and report $i$, train and test accuracy of the best model. Also, plot test and train accuracy for $2\\leq i\\leq100$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tfKS-Jq0-v4P"
   },
   "outputs": [],
   "source": [
    "# Your code goes here!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BWV0YUgRGg1p"
   },
   "source": [
    "**Question:** Analyze the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Answer:**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
